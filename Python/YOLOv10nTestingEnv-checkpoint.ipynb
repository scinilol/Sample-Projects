{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f23c26cd-9642-405a-89c2-07a5eef19476",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running YOLOv8 on your video video. Press 'Q' or close window to exit.\n",
      "\n",
      "0: 384x640 (no detections), 10.0ms\n",
      "Speed: 35.1ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.3ms\n",
      "Speed: 1.7ms preprocess, 8.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.7ms\n",
      "Speed: 1.7ms preprocess, 8.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.2ms\n",
      "Speed: 1.8ms preprocess, 9.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.7ms\n",
      "Speed: 1.9ms preprocess, 8.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 10.1ms\n",
      "Speed: 1.9ms preprocess, 10.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.2ms\n",
      "Speed: 1.7ms preprocess, 8.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 11.3ms\n",
      "Speed: 1.8ms preprocess, 11.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.3ms\n",
      "Speed: 1.8ms preprocess, 8.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.1ms\n",
      "Speed: 1.9ms preprocess, 8.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 11.1ms\n",
      "Speed: 1.9ms preprocess, 11.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 25.4ms\n",
      "Speed: 2.2ms preprocess, 25.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 drone, 25.5ms\n",
      "Speed: 2.1ms preprocess, 25.5ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 drone, 25.8ms\n",
      "Speed: 2.2ms preprocess, 25.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 25.6ms\n",
      "Speed: 2.2ms preprocess, 25.6ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 26.0ms\n",
      "Speed: 2.1ms preprocess, 26.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 drone, 26.6ms\n",
      "Speed: 2.2ms preprocess, 26.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 26.4ms\n",
      "Speed: 2.1ms preprocess, 26.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 26.8ms\n",
      "Speed: 2.4ms preprocess, 26.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 27.0ms\n",
      "Speed: 2.3ms preprocess, 27.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 26.9ms\n",
      "Speed: 2.4ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 drone, 27.5ms\n",
      "Speed: 2.4ms preprocess, 27.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 27.8ms\n",
      "Speed: 2.3ms preprocess, 27.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 drone, 28.0ms\n",
      "Speed: 2.5ms preprocess, 28.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 drone, 28.0ms\n",
      "Speed: 2.5ms preprocess, 28.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 drone, 27.8ms\n",
      "Speed: 2.5ms preprocess, 27.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 drone, 28.3ms\n",
      "Speed: 2.4ms preprocess, 28.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 drone, 28.5ms\n",
      "Speed: 2.2ms preprocess, 28.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 29.0ms\n",
      "Speed: 2.4ms preprocess, 29.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 drone, 29.1ms\n",
      "Speed: 2.6ms preprocess, 29.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 drone, 29.9ms\n",
      "Speed: 2.3ms preprocess, 29.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 29.6ms\n",
      "Speed: 2.5ms preprocess, 29.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 drone, 58.8ms\n",
      "Speed: 3.0ms preprocess, 58.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 59.3ms\n",
      "Speed: 3.9ms preprocess, 59.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 59.6ms\n",
      "Speed: 3.0ms preprocess, 59.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 55.5ms\n",
      "Speed: 2.9ms preprocess, 55.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 55.4ms\n",
      "Speed: 3.1ms preprocess, 55.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 55.1ms\n",
      "Speed: 3.3ms preprocess, 55.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 55.4ms\n",
      "Speed: 3.2ms preprocess, 55.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 21.9ms\n",
      "Speed: 2.4ms preprocess, 21.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 21.8ms\n",
      "Speed: 2.5ms preprocess, 21.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 21.8ms\n",
      "Speed: 2.4ms preprocess, 21.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 8.2ms\n",
      "Speed: 2.0ms preprocess, 8.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 8.6ms\n",
      "Speed: 1.9ms preprocess, 8.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 14.5ms\n",
      "Speed: 2.0ms preprocess, 14.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 11.3ms\n",
      "Speed: 3.3ms preprocess, 11.3ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 8.3ms\n",
      "Speed: 4.0ms preprocess, 8.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 8.1ms\n",
      "Speed: 1.9ms preprocess, 8.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 8.3ms\n",
      "Speed: 1.7ms preprocess, 8.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 11.2ms\n",
      "Speed: 1.8ms preprocess, 11.2ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 drone, 10.9ms\n",
      "Speed: 1.8ms preprocess, 10.9ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.4ms\n",
      "Speed: 1.7ms preprocess, 10.4ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.3ms\n",
      "Speed: 1.6ms preprocess, 10.3ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.6ms\n",
      "Speed: 1.7ms preprocess, 10.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 11.0ms\n",
      "Speed: 1.8ms preprocess, 11.0ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.6ms\n",
      "Speed: 1.7ms preprocess, 10.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 8.0ms\n",
      "Speed: 1.8ms preprocess, 8.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 9.8ms\n",
      "Speed: 1.7ms preprocess, 9.8ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.4ms\n",
      "Speed: 1.7ms preprocess, 10.4ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 8.0ms\n",
      "Speed: 1.9ms preprocess, 8.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.0ms\n",
      "Speed: 1.7ms preprocess, 10.0ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.4ms\n",
      "Speed: 1.7ms preprocess, 10.4ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.6ms\n",
      "Speed: 1.7ms preprocess, 10.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.0ms\n",
      "Speed: 1.8ms preprocess, 10.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.2ms\n",
      "Speed: 1.7ms preprocess, 10.2ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.0ms\n",
      "Speed: 1.7ms preprocess, 10.0ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 7.8ms\n",
      "Speed: 2.0ms preprocess, 7.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.4ms\n",
      "Speed: 1.8ms preprocess, 10.4ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.6ms\n",
      "Speed: 1.7ms preprocess, 10.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 8.0ms\n",
      "Speed: 1.8ms preprocess, 8.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 25.9ms\n",
      "Speed: 2.1ms preprocess, 25.9ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 25.6ms\n",
      "Speed: 2.2ms preprocess, 25.6ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 72.9ms\n",
      "Speed: 2.3ms preprocess, 72.9ms inference, 17.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 52.1ms\n",
      "Speed: 2.2ms preprocess, 52.1ms inference, 28.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 13.3ms\n",
      "Speed: 6.5ms preprocess, 13.3ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 11.7ms\n",
      "Speed: 1.9ms preprocess, 11.7ms inference, 11.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 13.4ms\n",
      "Speed: 4.0ms preprocess, 13.4ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 11.2ms\n",
      "Speed: 1.6ms preprocess, 11.2ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 11.5ms\n",
      "Speed: 2.2ms preprocess, 11.5ms inference, 8.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.1ms\n",
      "Speed: 3.4ms preprocess, 10.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.5ms\n",
      "Speed: 1.7ms preprocess, 10.5ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.6ms\n",
      "Speed: 1.6ms preprocess, 10.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 8.5ms\n",
      "Speed: 1.8ms preprocess, 8.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 9.8ms\n",
      "Speed: 1.7ms preprocess, 9.8ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 13.4ms\n",
      "Speed: 2.4ms preprocess, 13.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.2ms\n",
      "Speed: 1.7ms preprocess, 10.2ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 9.6ms\n",
      "Speed: 1.7ms preprocess, 9.6ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 9.5ms\n",
      "Speed: 1.7ms preprocess, 9.5ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.7ms\n",
      "Speed: 2.0ms preprocess, 10.7ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.6ms\n",
      "Speed: 1.6ms preprocess, 10.6ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 8.2ms\n",
      "Speed: 1.7ms preprocess, 8.2ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 8.6ms\n",
      "Speed: 1.6ms preprocess, 8.6ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 8.6ms\n",
      "Speed: 1.6ms preprocess, 8.6ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 8.6ms\n",
      "Speed: 1.7ms preprocess, 8.6ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 8.5ms\n",
      "Speed: 1.7ms preprocess, 8.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 10.1ms\n",
      "Speed: 2.5ms preprocess, 10.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 66.5ms\n",
      "Speed: 2.2ms preprocess, 66.5ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 9.7ms\n",
      "Speed: 1.9ms preprocess, 9.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 9.2ms\n",
      "Speed: 1.8ms preprocess, 9.2ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 8.4ms\n",
      "Speed: 1.7ms preprocess, 8.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 7.9ms\n",
      "Speed: 1.9ms preprocess, 7.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 8.6ms\n",
      "Speed: 1.7ms preprocess, 8.6ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 drones, 8.1ms\n",
      "Speed: 1.7ms preprocess, 8.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      " Tracking complete!\n",
      "Metrics saved to: D:/Anaconda-Projects/yolo_env/runs/train/drone_detector_50epoch/Metrics-Testing\\metrics_10302025_210847\\metrics-of-video.txt\n",
      "Preview video saved to: D:/Anaconda-Projects/yolo_env/runs/train/drone_detector_50epoch/Metrics-Testing\\metrics_10302025_210847\\preview_of_video10302025_210847.mov\n"
     ]
    }
   ],
   "source": [
    "# YOLOv8s-TestingEnv.ipynb\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"   # avoids MKL crashes on some Windows setups\n",
    "torch.set_num_threads(1)                      # keeps CPU thread count sane for Jupyter\n",
    "\n",
    "# show bounding-boxes only when confidence >= confMin\n",
    "confMin = 0.45\n",
    "\n",
    "# #Click-to-Focus settings\n",
    "# FOCUS_SECONDS = 1.0          # how long to keep focus after a click\n",
    "# FOCUS_SIZE_FRAC = 0.18       # ROI box width/height as a fraction of min(frame_w, frame_h)\n",
    "# _click_until_ts = 0.0\n",
    "# _click_xy = None\n",
    "# _click_log = []              # <<< NEW: collect click logs\n",
    "\n",
    "# # Mouse callback\n",
    "# def _mouse_cb(event, x, y, flags, param):\n",
    "#     global _click_until_ts, _click_xy\n",
    "#     if event == cv2.EVENT_LBUTTONDOWN:\n",
    "#         _click_xy = (x, y)\n",
    "#         _click_until_ts = time.time() + FOCUS_SECONDS\n",
    "#         ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#         _click_log.append((ts, x, y))   # store; we’ll flush to file each frame\n",
    "#         print(f\"[click] {ts}  x={x} y={y}\")\n",
    "#         # visual feedback: small crosshair will be drawn in the loop\n",
    "#         # (no blocking here)\n",
    "\n",
    "\n",
    "# Load trained model from YOLOv8s-Trainable\n",
    "model = YOLO(\"D:/Anaconda-Projects/yolo_env/runs/train/drone_detector_50epoch/weights/best.pt\")\n",
    "\n",
    "# Choose a video to test\n",
    "video_path = \"D:/Anaconda-Projects/yolo_env/Drone-Dataset/archive/15MAY22 UAV Videos 4k/20220121_110507_VIS_H264.MOV\"\n",
    "#Create output directory for metrics inside trained model\n",
    "metrics_dir = \"D:/Anaconda-Projects/yolo_env/runs/train/drone_detector_50epoch/Metrics-Testing\"\n",
    "timestamp = datetime.now().strftime(\"%m%d%Y_%H%M%S\")\n",
    "session_dir = os.path.join(metrics_dir, f\"metrics_{timestamp}\")\n",
    "os.makedirs(session_dir, exist_ok=True)\n",
    "#click_log_path = os.path.join(session_dir, \"click_log.txt\")\n",
    "\n",
    "#Open video file\n",
    "vid = cv2.VideoCapture(video_path)\n",
    "if not vid.isOpened():\n",
    "     print(f\"Failed to open video: {video_path}\")\n",
    "     exit()\n",
    "\n",
    "# Get video properties for saving output\n",
    "# fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "# width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "# height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define codec and create VideoWriter\n",
    "# output_video_path = os.path.join(output_dir, f\"tracked_output_{timestamp}.mp4\")\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "# out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "    \n",
    "print(\"Running YOLOv8 on your video video. Press 'Q' or close window to exit.\")\n",
    "# print(f\"Output will be saved to: {output_dir}\")\n",
    "\n",
    "# Create window\n",
    "cv2.namedWindow(\"YOLOv8 Drone Tracking\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"YOLOv8 Drone Tracking\", 960, 540)\n",
    "\n",
    "frame_count = 0\n",
    "total_detections = 0\n",
    "\n",
    "# BEFORE the loop\n",
    "start_time = time.time()\n",
    "out = None\n",
    "preview_of_video = os.path.join(session_dir, f\"preview_of_video{timestamp}.mov\")\n",
    "\n",
    "try:\n",
    "    while vid.isOpened():\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        h, w = frame.shape[:2]\n",
    "\n",
    "        # lazy-init preview writer once we know FPS and size\n",
    "        if out is None:\n",
    "            fps = vid.get(cv2.CAP_PROP_FPS)\n",
    "            if not fps or fps <= 0:\n",
    "                fps = 60.0  # fallback if source FPS not reported\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # works on most OpenCV builds for .mov\n",
    "            out = cv2.VideoWriter(preview_of_video, fourcc, fps, (w, h))\n",
    "\n",
    "        # Build focused frame if click is active\n",
    "        #use_focus = time.time() < _click_until_ts and _click_xy is not None\n",
    "        # if use_focus:\n",
    "        #     fx, fy = _click_xy\n",
    "        #     side = int(min(w, h) * FOCUS_SIZE_FRAC)\n",
    "        #     x0 = max(fx - side // 2, 0)\n",
    "        #     y0 = max(fy - side // 2, 0)\n",
    "        #     x1 = min(x0 + side, w)\n",
    "        #     y1 = min(y0 + side, h)\n",
    "        #     # keep only ROI content; black everywhere else (same size image)\n",
    "        #     focused = np.zeros_like(frame)\n",
    "        #     focused[y0:y1, x0:x1] = frame[y0:y1, x0:x1]\n",
    "        #     source_img = focused\n",
    "        # else:\n",
    "        source_img = frame\n",
    "        #Run tracking on frame/interface\n",
    "        results = model.track(\n",
    "            # Tracker configuration YAML file (controls which algorithm to use).\n",
    "            tracker=\"bytetrack.yaml\",   # Options: 'bytetrack.yaml', 'botsort.yaml', 'ocsort.yaml'or 'botsort.yaml'\n",
    "    \n",
    "            # Path to the video, image folder, webcam index (0), or stream URL.\n",
    "            source=source_img, # Examples: \"video.mp4\", 0, \"rtsp://...\", \"folder_of_images/\"\n",
    "    \n",
    "            # Verbosity control: print logs (True) or suppress (False).\n",
    "            verbose=True,\n",
    "    \n",
    "            # Whether to display the results in a pop-up OpenCV window.\n",
    "            show=False,\n",
    "    \n",
    "            # shows frames in a pop-up window\n",
    "            # When True, saves to: runs/track/{name}/\n",
    "            save=False,\n",
    "\n",
    "            persist=True,                   # keeps tracking IDs between frames\n",
    "    \n",
    "            # saves output under runs/detect/\n",
    "            # Minimum confidence threshold for detection (0.0–1.0).\n",
    "            # Detections below this confidence are ignored.\n",
    "            conf=confMin,                        # confidence threshold\n",
    "    \n",
    "            # Set image size for inference. Smaller = faster but less accurate.\n",
    "            # Typical values: 640, 720, 1080\n",
    "            #imgsz=640,                       #smaller frame size\n",
    "    \n",
    "            # IoU (Intersection over Union) threshold for non-max suppression (0.0–1.0).\n",
    "            # Higher = fewer overlapping boxes, lower = more.\n",
    "            #iou=0.7,\n",
    "    \n",
    "            # Directory where outputs (video, logs, etc.) are stored.\n",
    "            project=\"D:/Anaconda-Projects/yolo_env/runs/train/drone_detector\",\n",
    "    \n",
    "            # Name of the subfolder under 'project' where this run’s files go.\n",
    "            # Combined path example: D:/Anaconda-Projects/YOLO-Outputs/drone_tracking/\n",
    "            name=\"drone_tracking\",\n",
    "    \n",
    "            # If True, overwrites existing folders with the same name instead of creating 'drone_tracking2', etc.\n",
    "            exist_ok=False,\n",
    "    \n",
    "            # Use GPU acceleration if available (default True). Set False for CPU-only.\n",
    "            device=\"cuda:0\",  # examples: 'cpu', 'cuda:0', 'cuda:1'\n",
    "            # Whether to visualize model predictions inline (useful in notebooks).\n",
    "            # Usually disabled when 'show=True'.\n",
    "            visualize=False,\n",
    "    \n",
    "            # Whether to return tracking results as a generator instead of accumulating all in memory.\n",
    "            # Prevents out-of-memory errors for long videos or streams.\n",
    "            stream=False,\n",
    "    \n",
    "            # Maximum number of detections per image.\n",
    "            max_det=300,\n",
    "    \n",
    "            # Classes to detect (list of class IDs). e.g., [0] for 'person', [2, 3, 5] for specific objects.\n",
    "            # None = detect all classes.\n",
    "            classes=None,\n",
    "    \n",
    "            # Enable or disable saving of cropped detections.\n",
    "            save_crop=False,\n",
    "    \n",
    "            # Save text labels (bounding box coordinates and class IDs) to *.txt files.\n",
    "            save_txt=False,\n",
    "    \n",
    "            # Line thickness for bounding boxes in the display.\n",
    "            line_width=2,\n",
    "    \n",
    "            # Whether to automatically open the output folder after completion.\n",
    "            show_labels=False,\n",
    "    \n",
    "            # Control tracker-specific behavior (inside the YAML config).\n",
    "            # Examples: tracking buffer size, re-ID thresholds, frame rate smoothing, etc.\n",
    "            )\n",
    "        #Extra safety: filter any boxes below confMin (some trackers can pass along extra boxes)\n",
    "        r = results[0]\n",
    "        if r.boxes is not None and len(r.boxes) > 0:\n",
    "            confs = r.boxes.conf\n",
    "            keep = confs >= confMin\n",
    "            r.boxes = r.boxes[keep]\n",
    "\n",
    "        # # Draw on the ORIGINAL frame\n",
    "        # annotated = frame.copy()\n",
    "        # if r.boxes is not None and len(r.boxes) > 0:\n",
    "        #     names = r.names if hasattr(r, \"names\") else {}\n",
    "        #     for b in r.boxes:\n",
    "        #         x1, y1, x2, y2 = map(int, b.xyxy[0].tolist())\n",
    "        #         conf = float(b.conf[0])\n",
    "        #         cls_id = int(b.cls[0]) if b.cls is not None else -1\n",
    "        #         label = f\"{names.get(cls_id, 'obj')} {conf:.2f}\"\n",
    "        #         cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        #         cv2.putText(annotated, label, (x1, max(10, y1 - 6)),\n",
    "        #                     cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "        # # crosshair to show focus point (optional)\n",
    "        # if use_focus and _click_xy is not None:\n",
    "        #     fx, fy = _click_xy\n",
    "        #     cv2.drawMarker(annotated, (fx, fy), (255, 255, 0),\n",
    "        #                    markerType=cv2.MARKER_CROSS, markerSize=12, thickness=2)\n",
    "         # Draw results\n",
    "        annotated = results[0].plot()\n",
    "        cv2.imshow(\"YOLOv8 Drone Tracking\", annotated)\n",
    "\n",
    "        # write annotated frame to preview\n",
    "        if out is not None:\n",
    "            out.write(annotated)\n",
    "        \n",
    "        # # Write any new click entries to file (non-blocking)\n",
    "        # if _click_log:\n",
    "        #     with open(click_log_path, \"a\") as f:\n",
    "        #         while _click_log:\n",
    "        #             ts, cx, cy = _click_log.pop(0)\n",
    "        #             roi_txt = f\"{roi_box}\" if roi_box else \"None\"\n",
    "        #             f.write(f\"{ts}, click=({cx},{cy}), roi={roi_txt}\\n\")\n",
    "\n",
    "        # Collect metrics\n",
    "        frame_count += 1\n",
    "        total_detections += (len(r.boxes) if r.boxes is not None else 0)\n",
    "        \n",
    "        # Visualize results on frame\n",
    "        #frame = results[0].plot()\n",
    "        \n",
    "        # Resize for display\n",
    "        #display_frame = cv2.resize(annotated_frame, (960, 540))\n",
    "        \n",
    "        # Write frame to output video\n",
    "        #out.write(annotated_frame)\n",
    "        \n",
    "        # Display frame\n",
    "        #cv2.imshow(\"YOLOv8 Drone Tracking\", display_frame)\n",
    "        \n",
    "        # Break on 'Q' press or window close\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q') or cv2.getWindowProperty(\"YOLOv8 Drone Tracking\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "            break\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    #Cleanup\n",
    "    vid.release()\n",
    "    if out is not None:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Compute runtime stats\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    avg_fps = frame_count / total_time if total_time > 0 else 0\n",
    "    avg_detections = total_detections / frame_count if frame_count > 0 else 0\n",
    "\n",
    "    #session_metrics = os.path.join(session_dir, \"metrics-of-video.txt\")\n",
    "    tracker_used = \"bytetrack.yaml\"      # same one you pass to model.track()\n",
    "    conf_threshold = confMin                # same confidence level\n",
    "    device_used = \"cuda:0\"               # same device\n",
    "    img_size = 640                       # model input size\n",
    "\n",
    "    # # Save metrics\n",
    "    metrics_file = os.path.join(session_dir, \"metrics-of-video.txt\")\n",
    "    \n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        f.write(f\"Video: {os.path.basename(video_path)}\\n\")\n",
    "        f.write(f\"Frames processed: {frame_count}\\n\")\n",
    "        f.write(f\"Total detections: {total_detections}\\n\")\n",
    "        f.write(f\"Average detections per frame: {avg_detections:.2f}\\n\")\n",
    "        f.write(f\"Average FPS: {avg_fps:.2f}\\n\")\n",
    "        f.write(f\"Total runtime (s): {total_time:.2f}\\n\")\n",
    "        f.write(f\"Model path: {model.ckpt_path if hasattr(model, 'ckpt_path') else 'N/A'}\\n\")\n",
    "        f.write(f\"Model name: {os.path.basename(model.ckpt_path) if hasattr(model, 'ckpt_path') else 'best.pt'}\\n\")\n",
    "        f.write(f\"Tracker used: {os.path.basename(tracker_used)}\\n\")\n",
    "        f.write(f\"Confidence threshold: {conf_threshold}\\n\")\n",
    "        f.write(f\"Device: {device_used}\\n\")\n",
    "        f.write(f\"Image size: {img_size}\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "        #f.write(f\"ClickLog: {click_log_path}\\n\")\n",
    "\n",
    "    print(\"\\n Tracking complete!\")\n",
    "    print(f\"Metrics saved to: {metrics_file}\")\n",
    "    print(f\"Preview video saved to: {preview_of_video}\")\n",
    "    #print(f\"Click log saved to: {click_log_path}\")\n",
    "    # print(f\"Video processing complete!\")\n",
    "    # print(f\"Output video saved to: {output_video_path}\")\n",
    "    # print(f\"Metrics directory: {output_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "# Define video writer\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#out = cv2.VideoWriter('D:/Anaconda-Projects/yolo_env/runs/train/drone_detector/YOLOv8s-outputTesting.mp4', \n",
    "#                      fourcc, \n",
    "#                     30.0, \n",
    "#                      (int(cap.get(3)), int(cap.get(4))))\n",
    "        # Load your trained model\n",
    "        #model = YOLO(\"D:/Anaconda-Projects/yolo_env/runs/train/drone_detector/weights/best.pt\")\n",
    "        \n",
    "        # Run detection\n",
    "        # results = model.predict(\n",
    "        #     source=\"Drone-Dataset/archive/15MAY22 UAV Videos 4k/20220121_101736_VIS_H264.MOV\",\n",
    "        #     conf=0.25,\n",
    "        #     show=True\n",
    "        # )\n",
    "    \n",
    "# annotated = results[0].plot()\n",
    "# resized = cv2.resize(annotated, (960, 540))\n",
    "# cv2.imshow(\"YOLOv8s Drone Tracking\", resized)\n",
    "    \n",
    "# Press 'q' or close window to exit safely\n",
    "# key = cv2.waitKey(1) & 0xFF\n",
    "# if key == ord('q') or cv2.getWindowProperty(\"YOLOv8s Drone Tracking\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "#     break\n",
    "\n",
    "\n",
    "#plt.imshow(results[0].plot()) # draws bounding boxes\n",
    "#plt.axis('off')\n",
    "#plt.show()\n",
    "#results[0].boxes.data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLO Env",
   "language": "python",
   "name": "yolo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
