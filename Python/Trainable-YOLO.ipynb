from ultralytics import YOLO
from IPython.display import Video
import cv2
import os
import pandas as pd
import torch

# DISCLAIMER:
# This script is best used when training higher epoch(s) (recommended)
# That way you have less issues with resume_training function

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"   # avoids MKL crashes on some Windows setups
torch.set_num_threads(1)                      # keeps CPU thread count sane for Jupyter

# set to True if you want to resume training from a previous model
resume_training = False
# path to csv metrics
csv_path = "PATH/TO/CSV/Metrics"

# path to your previously trained model weights for 'best' and 'last'
previous_model_path_best = "PATH/TO/best.pt"
previous_model_path_last = "PATH/TO/last.pt"

# new dataset path for retraining (you can keep same one or point to new)
# Hard to avoid conflicts if you change data sets when resume_training = True
dataset_path = "PATH/TO/data.yaml"

# total desired epochs overall
target_epochs = 50

if resume_training:
    print("Resuming training from previous model...")
    model = YOLO(previous_model_path_best)
    
    results = model.train(
        data=dataset_path,       # data set path
        tracker="bytetrack.yaml",   # Options: 'bytetrack.yaml', 'botsort.yaml', 'ocsort.yaml'
        epochs=target_epochs,              # increase for better accuracy
        imgsz=640,              # image size
        batch=4,                # adjust depending on VRAM
        plots=False,            # no matplotlib plotting during train
        amp=False,             # turn off mixed precision if your driver/CUDA combo is touchy
        device="cuda:0",        # Use GPU acceleration if available (default True). Set False for CPU-only.
        workers=0,              # disables multiprocessing
        cache=False,            # disable pre-caching (keeps memory stable)
        deterministic=True,     # prevents race conditions
        resume=False,            # continue from previous checkpoints
        #lr0=0.0005,                # lower learning rate to prevent NaN explosion
        exist_ok=True,
        name="drone_detector",  # output folder name
        project="D:/Anaconda-Projects/yolo_env/runs/train",
    )
else:
    print("Starting new training from yolov8s.pt...")
    # Load pretrained YOLOv8s model
    model = YOLO(previous_model_path_best)
    results = model.train(
        data=dataset_path,
        # Tracker configuration YAML file (controls which algorithm to use)
        tracker="bytetrack.yaml",   # Options: 'bytetrack.yaml', 'botsort.yaml', 'ocsort.yaml'
        epochs=target_epochs,              # increase for better accuracy
        imgsz=640,              # image size
        batch=6,                # adjust depending on VRAM
        device="cuda:0",        # Use GPU acceleration if available (default True). Set False for CPU-only.
        exist_ok=True,
        workers=0,              # critical fix: disables multiprocessing
        cache=False,            # disable pre-caching (keeps memory stable)
        resume=False,            # continue from previous checkpoints
        amp=False,             # turn off mixed precision if your driver/CUDA combo is touchy
        deterministic=True,     # prevents race conditions
        name="drone_detector_50epoch",  # output folder name
        project="D:/Anaconda-Projects/yolo_env/runs/train",
    )

# # Optional: print metrics
# metrics = results.results_dict
# print("\nTraining Results:")
# print(f"mAP50: {metrics.get('metrics/mAP50(B)', 'N/A')}")
# print(f"Precision: {metrics.get('metrics/precision(B)', 'N/A')}")
# print(f"Recall: {metrics.get('metrics/recall(B)', 'N/A')}")

    #Run YOLO inference
    #results = model.predict(
            # Tracker configuration YAML file (controls which algorithm to use).
            # Options: 'bytetrack.yaml', 'botsort.yaml', 'ocsort.yaml'
            ##tracker="bytetrack.yaml",   # or 'botsort.yaml'
            # Path to the video, image folder, webcam index (0), or stream URL.
            # Examples: "video.mp4", 0, "rtsp://...", "folder_of_images/"
            #source=frame,
            # Verbosity control: print logs (True) or suppress (False).
            #verbose=False,
            # Whether to display the results in a pop-up OpenCV window.
            #show=True,                       # shows frames in a pop-up window
            # When True, saves to: runs/track/{name}/
            #save=False,                       # saves output under runs/detect/
            # Minimum confidence threshold for detection (0.0–1.0).
            # Detections below this confidence are ignored.
            #conf=0.25,                        # confidence threshold
            # Set image size for inference. Smaller = faster but less accurate.
            # Typical values: 640, 720, 1080
            #imgsz=640,                       #smaller frame size
            # IoU (Intersection over Union) threshold for non-max suppression (0.0–1.0).
            # Higher = fewer overlapping boxes, lower = more.
            #iou=0.7,
            # Directory where outputs (video, logs, etc.) are stored.
            #project="D:/Anaconda-Projects/YOLO-Outputs",
            # Name of the subfolder under 'project' where this run’s files go.
            # Combined path example: D:/Anaconda-Projects/YOLO-Outputs/drone_tracking/
            #name="drone_tracking",
            # If True, overwrites existing folders with the same name instead of creating 'drone_tracking2', etc.
            #exist_ok=False,
            # Use GPU acceleration if available (default True). Set False for CPU-only.
            #device=cuda:0,  # examples: 'cpu', 'cuda:0', 'cuda:1'
            # Whether to visualize model predictions inline (useful in notebooks).
            # Usually disabled when 'show=True'.
            #visualize=False,
            # Whether to return tracking results as a generator instead of accumulating all in memory.
            # Prevents out-of-memory errors for long videos or streams.
            #stream=False,
            # Maximum number of detections per image.
            #max_det=300,
            # Classes to detect (list of class IDs). e.g., [0] for 'person', [2, 3, 5] for specific objects.
            # None = detect all classes.
            #classes=None,
            # Enable or disable saving of cropped detections.
            #save_crop=False,
            # Save text labels (bounding box coordinates and class IDs) to *.txt files.
            #save_txt=False,
            # Line thickness for bounding boxes in the display.
            #line_width=2,
            # Whether to automatically open the output folder after completion.
            #show_labels=False,
            # Control tracker-specific behavior (inside the YAML config).
            # Examples: tracking buffer size, re-ID thresholds, frame rate smoothing, etc.
        #)

# Print detection summary
print("Training finished!!!")
print("Training finished!")
print(f"Model saved under: {model.trainer.save_dir}")
